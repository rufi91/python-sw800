{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence\n",
    "\n",
    "Often we deal with sets in applied machine learning such as a train or test set of samples. Each sample in the set can be thought of as an observation from the domain. In a set, the order of the observations is not important.\n",
    "\n",
    "A sequence is different. The sequence imposes an explicit order on the observations. The order is important. It must be respected in the formulation of prediction problems that use the sequence data as input or output for the model.\n",
    "\n",
    "## Sequence Prediction\n",
    "\n",
    "Sequence prediction involves predicting the next value for a given input sequence. For example:\n",
    "\n",
    "Input Sequence: 1, 2, 3, 4, 5\n",
    "\n",
    "Output Sequence: 6\n",
    "\n",
    "Some examples of sequence prediction problems include:\n",
    "\n",
    " Weather Forecasting. Given a sequence of observations about the weather over time, predict the expected weather tomorrow.\n",
    "\n",
    " Stock Market Prediction. Given a sequence of movements of a security over time, predict the next movement of the security.\n",
    "\n",
    " Product Recommendation. Given a sequence of past purchases for a customer, predictthe next purchase for a customer.\n",
    "\n",
    "## Sequence Classification\n",
    "\n",
    "Sequence classification involves predicting a class label for a given input sequence. For example:\n",
    "\n",
    "Input Sequence: 1, 2, 3, 4, 5\n",
    "\n",
    "Output Sequence: \"good\"\n",
    "\n",
    "Some examples of sequence classification problems include:\n",
    "\n",
    " DNA Sequence Classification. Given a DNA sequence of A, C, G, and T values, predict whether the sequence is for a coding or non-coding region.\n",
    "\n",
    " Anomaly Detection. Given a sequence of observations, predict whether the sequence is anomalous or not.\n",
    "\n",
    " Sentiment Analysis. Given a sequence of text such as a review or a tweet, predict whether the sentiment of the text is positive or negative.\n",
    "\n",
    "## Sequence Generation\n",
    "\n",
    "Sequence generation involves generating a new output sequence that has the same general characteristics as other sequences in the corpus. For example:\n",
    "\n",
    "Input Sequence: [1, 3, 5], [7, 9, 11]\n",
    "\n",
    "Output Sequence: [3, 5 ,7]\n",
    "\n",
    "Some examples of sequence generation problems include:\n",
    "\n",
    " Text Generation. Given a corpus of text, such as the works of Shakespeare, generate new sentences or paragraphs of text that read they could have been drawn from the corpus.\n",
    "\n",
    " Handwriting Prediction. Given a corpus of handwriting examples, generate handwriting for new phrases that has the properties of handwriting in the corpus.\n",
    "\n",
    "Music Generation. Given a corpus of examples of music, generate new musical pieces that have the properties of the corpus.\n",
    "\n",
    "\n",
    "Sequence generation may also refer to the generation of a sequence given a single observation as input. An example is the automatic textual description of images.\n",
    "\n",
    "\n",
    " Image Caption Generation. Given an image as input, generate a sequence of words that describe an image.\n",
    "\n",
    "## Sequence-to-Sequence Prediction\n",
    "\n",
    "Sequence-to-sequence prediction involves predicting an output sequence given an input sequence. For example:\n",
    "\n",
    "Input Sequence: 1, 2, 3, 4, 5\n",
    "\n",
    "Output Sequence: 6, 7, 8, 9, 10\n",
    "\n",
    "Sequence-to-sequence prediction is a subtle but challenging extension of sequence prediction, where, rather than predicting a single next value in the sequence, a new sequence is predicted that may or may not have the same length or be of the same time as the input sequence. This type of problem has recently seen a lot of study in the area of automatic text translation (e.g.\n",
    "translating English to French) and may be referred to by the abbreviation seq2seq.\n",
    "\n",
    "If the input and output sequences are a time series, then the problem may be referred to as\n",
    "multi-step time series forecasting. Some examples of sequence-to-sequence problems include:\n",
    "\n",
    " Multi-Step Time Series Forecasting. Given a time series of observations, predict a sequence of observations for a range of future time steps.\n",
    "\n",
    " Text Summarization. Given a document of text, predict a shorter sequence of text that describes the salient parts of the source document.\n",
    "\n",
    " Program Execution. Given the textual description program or mathematical equation predict the sequence of characters that describes the correct output.\n",
    "\n",
    "## LSTM Weights\n",
    "\n",
    "A memory cell has weight parameters for the input, output, as well as an internal state that is built up through exposure to input time steps.\n",
    "\n",
    " Input Weights. Used to weight input for the current time step.\n",
    "\n",
    " Output Weights. Used to weight the output from the last time step.\n",
    "\n",
    " Internal State. Internal state used in the calculation of the output for this time step.\n",
    "\n",
    "## LSTM Gates\n",
    "\n",
    "The key to the memory cell are the gates. These too are weighted functions that further govern the information flow in the cell. There are three gates:\n",
    "\n",
    " Forget Gate: Decides what information to discard from the cell.\n",
    "\n",
    " Input Gate: Decides which values from the input to update the memory state.\n",
    "\n",
    " Output Gate: Decides what to output based on input and the memory of the cell.\n",
    "\n",
    "The forget gate and input gate are used in the updating of the internal state. The output gate is a final limiter on what the cell actually outputs. It is these gates and the consistent data flow called the constant error carrousel or CEC that keep each cell stable (neither exploding or vanishing).\n",
    "\n",
    "## Applications of LSTMs\n",
    "\n",
    "Automatic Image Caption Generation\n",
    "\n",
    "Automatic Translation of Text\n",
    "\n",
    "Automatic Handwriting Generation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Prepare Data For LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Series Data (0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     10.0\n",
      "1     20.0\n",
      "2     30.0\n",
      "3     40.0\n",
      "4     50.0\n",
      "5     60.0\n",
      "6     70.0\n",
      "7     80.0\n",
      "8     90.0\n",
      "9    100.0\n",
      "dtype: float64\n",
      "Min: 10.000000, Max: 100.000000\n",
      "[[ 0.        ]\n",
      " [ 0.11111111]\n",
      " [ 0.22222222]\n",
      " [ 0.33333333]\n",
      " [ 0.44444444]\n",
      " [ 0.55555556]\n",
      " [ 0.66666667]\n",
      " [ 0.77777778]\n",
      " [ 0.88888889]\n",
      " [ 1.        ]]\n",
      "[[  10.]\n",
      " [  20.]\n",
      " [  30.]\n",
      " [  40.]\n",
      " [  50.]\n",
      " [  60.]\n",
      " [  70.]\n",
      " [  80.]\n",
      " [  90.]\n",
      " [ 100.]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# define contrived series\n",
    "data = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n",
    "series = Series(data)\n",
    "print(series)\n",
    "# prepare data for normalization\n",
    "values = series.values\n",
    "values = values.reshape((len(values), 1))\n",
    "# train the normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(values)\n",
    "print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n",
    "# normalize the dataset and print\n",
    "normalized = scaler.transform(values)\n",
    "print(normalized)\n",
    "# inverse transform and print\n",
    "inversed = scaler.inverse_transform(normalized)\n",
    "print(inversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Standardize Series Data(Mean=0, std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    5.5\n",
      "2    9.0\n",
      "3    2.6\n",
      "4    8.8\n",
      "5    3.0\n",
      "6    4.1\n",
      "7    7.9\n",
      "8    6.3\n",
      "dtype: float64\n",
      "Mean: 5.355556, StandardDeviation: 2.712568\n",
      "[[-1.60569456]\n",
      " [ 0.05325007]\n",
      " [ 1.34354035]\n",
      " [-1.01584758]\n",
      " [ 1.26980948]\n",
      " [-0.86838584]\n",
      " [-0.46286604]\n",
      " [ 0.93802055]\n",
      " [ 0.34817357]]\n",
      "[[ 1. ]\n",
      " [ 5.5]\n",
      " [ 9. ]\n",
      " [ 2.6]\n",
      " [ 8.8]\n",
      " [ 3. ]\n",
      " [ 4.1]\n",
      " [ 7.9]\n",
      " [ 6.3]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "# define contrived series\n",
    "data = [1.0, 5.5, 9.0, 2.6, 8.8, 3.0, 4.1, 7.9, 6.3]\n",
    "series = Series(data)\n",
    "print(series)\n",
    "# prepare data for normalization\n",
    "values = series.values\n",
    "values = values.reshape((len(values), 1))\n",
    "# train the normalization\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(values)\n",
    "print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, sqrt(scaler.var_)))\n",
    "# normalize the dataset and print\n",
    "standardized = scaler.transform(values)\n",
    "print(standardized)\n",
    "# inverse transform and print\n",
    "inversed = scaler.inverse_transform(standardized)\n",
    "print(inversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Convert Categorical Data to Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold' 'cold' 'warm' 'cold' 'hot' 'hot' 'warm' 'cold' 'warm' 'hot']\n",
      "[0 0 2 0 1 1 2 0 2 1]\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]]\n",
      "['cold']\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sequences with Varied Lengths (Sequence Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Seq.Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [0 1 2 3]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# define sequences\n",
    "sequences = [\n",
    "[1, 2, 3, 4],\n",
    "[1, 2, 3],\n",
    "[1]\n",
    "]\n",
    "# pad sequence\n",
    "padded = pad_sequences(sequences)\n",
    "print(padded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Seq.Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [1 2 3 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# define sequences\n",
    "sequences = [\n",
    "[1, 2, 3, 4],\n",
    "[1, 2, 3],\n",
    "[1]\n",
    "]\n",
    "# pad sequence\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sequences with Varied Lengths (Seq. Truncation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Seq.Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4]\n",
      " [2 3]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# define sequences\n",
    "sequences = [\n",
    "[1, 2, 3, 4],\n",
    "[1, 2, 3],\n",
    "[1]\n",
    "]\n",
    "# truncate sequence\n",
    "truncated= pad_sequences(sequences, maxlen=2)\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Seq.Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [1 2]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# define sequences\n",
    "sequences = [\n",
    "[1, 2, 3, 4],\n",
    "[1, 2, 3],\n",
    "[1]\n",
    "]\n",
    "# truncate sequence\n",
    "truncated= pad_sequences(sequences, maxlen=2, truncating='post')\n",
    "print(truncated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas shift() Function\n",
    "A key function to help transform time series data into a supervised learning problem is the Pandas shift() function. Given a DataFrame, the shift() function can be used to create copies of columns that are pushed forward (rows of NaN values added to the front) or pulled back (rows of NaN values added to the end). This is the behavior required to create columns of lag observations as well as columns of forecast observations for a time series dataset in a supervised learning format. Let’s look at some examples of the shift() function in action. We can define a mock time series dataset as a sequence of 10 numbers, in this case a single column in a DataFrame as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   t\n",
      "0  0\n",
      "1  1\n",
      "2  2\n",
      "3  3\n",
      "4  4\n",
      "5  5\n",
      "6  6\n",
      "7  7\n",
      "8  8\n",
      "9  9\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "# define the sequence\n",
    "df = DataFrame()\n",
    "df['t'] = [x for x in range(10)]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   t  t-1\n",
      "0  0  NaN\n",
      "1  1  0.0\n",
      "2  2  1.0\n",
      "3  3  2.0\n",
      "4  4  3.0\n",
      "5  5  4.0\n",
      "6  6  5.0\n",
      "7  7  6.0\n",
      "8  8  7.0\n",
      "9  9  8.0\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "# define the sequence\n",
    "df = DataFrame()\n",
    "df['t'] = [x for x in range(10)]\n",
    "# shift forward\n",
    "df['t-1'] = df['t'].shift(1)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   t  t+1\n",
      "0  0  1.0\n",
      "1  1  2.0\n",
      "2  2  3.0\n",
      "3  3  4.0\n",
      "4  4  5.0\n",
      "5  5  6.0\n",
      "6  6  7.0\n",
      "7  7  8.0\n",
      "8  8  9.0\n",
      "9  9  NaN\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "# define the sequence\n",
    "df = DataFrame()\n",
    "df['t'] = [x for x in range(10)]\n",
    "# shift backward\n",
    "df['t+1'] = df['t'].shift(-1)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Develop LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lesson is to understand how to define, fit, and evaluate LSTM models using the Keras deep learning library in \n",
    "Python. After completing this lesson, you will know:\n",
    "\n",
    " How to define an LSTM model, including how to reshape your data for the required 3Dinput.\n",
    "\n",
    " How to fit and evaluate your LSTM model and use it to make predictions on new data.\n",
    "\n",
    " How to take fine-grained control over the internal state in the model and when it is reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "The first hidden layer in the network must define the number of inputs to expect, e.g. the shape of the input layer. Input must be three-dimensional, comprised of samples, time steps, and features in that order.\n",
    "\n",
    " Samples. These are the rows in your data. One sample may be one sequence.\n",
    "\n",
    " Time steps. These are the past observations for a feature, such as lag variables.\n",
    "\n",
    " Features. These are columns in your data.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(5, input_shape=(2,1)))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "The number of samples does not have to be specified. The model assumes one or more samples, leaving you to define only the number of time steps and features. The final section of this lesson provides additional examples of preparing input data for LSTM models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "The choice of activation function is most important for the output layer as it will define the format that predictions will take. For example, below are some common predictive modeling problem types and the structure and standard activation function that you can use in the output layer:\n",
    "\n",
    " Regression: Linear activation function, or linear, and the number of neurons matching the number of outputs. This is the default activation function used for neurons in the Dense layer.\n",
    "\n",
    " Binary Classification (2 class): Logistic activation function, or sigmoid, and one neuron the output layer.\n",
    "\n",
    " Multiclass Classification (> 2 class): Softmax activation function, or softmax, and one output neuron per class value, assuming a one hot encoded output pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "or\n",
    "\n",
    "algorithm = SGD(lr=0.1, momentum=0.3)\n",
    "\n",
    "model.compile(optimizer=algorithm, loss='mse')\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The type of predictive modeling problem imposes constraints on the type of loss function that can be used. For example, below are some standard loss functions for different predictive model types:\n",
    "\n",
    " Regression: Mean Squared Error or mean squared error, mse for short.\n",
    "\n",
    " Binary Classification (2 class): Logarithmic Loss, also called cross entropy or binary crossentropy.\n",
    "\n",
    " Multiclass Classification (> 2 class): Multiclass Logarithmic Loss or categorical crossentropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "verbose = 0 turns of the progress bar\n",
    "\n",
    "predictions = model.predict(X)           # Probality\n",
    "\n",
    "predictions = model.predict_classes(X)   # Class labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LSTM State Management\n",
    "Each LSTM memory unit maintains internal state that is accumulated. This internal state may require careful management for your sequence prediction problem both during the training of the network and when making predictions. By default, the internal state of all LSTM memory units in the network is reset after each batch, e.g. when the network weights are updated. This means that the configuration of the batch size imposes a tension between three things:\n",
    "\n",
    " The efficiency of learning, or how many samples are processed before an update.\n",
    "\n",
    " The speed of learning, or how often weights are updated.\n",
    "\n",
    " The influence of internal state, or how often internal state is reset.\n",
    "\n",
    "Keras provides flexibility to decouple the resetting of internal state from updates to network weights by defining an LSTM layer as stateful. This can be done by setting the stateful argument on the LSTM layer to True. When stateful LSTM layers are used, you must also define the batch size as part of the input shape in the definition of the network by setting the batch input shape argument and the batch size must be a factor of the number of samples in the training dataset. The batch input shape argument requires a 3-dimensional tuple defined as batch size, time steps, and features. For example, we can define a stateful LSTM to be trained on a training dataset with 100 samples, a batch size of 10, and 5 time steps for 1 feature, as follows.\n",
    "\n",
    "model.add(LSTM(2, stateful=True, batch_input_shape=(10, 5, 1)))\n",
    "\n",
    "A stateful LSTM will not reset the internal state at the end of each batch. Instead, you have fine grained control over when to reset the internal state by calling the reset states() function. For example, we may want to reset the internal state at the end of each single epoch which we could do as follows:\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "model.fit(X, y, epochs=1, batch_input_shape=(10, 5, 1))\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "The same batch size used in the definition of the stateful LSTM must also be used when making predictions.\n",
    "\n",
    "predictions = model.predict(X, batch_size=10)\n",
    "\n",
    "\n",
    "To make this more concrete, below are a 3 common examples for managing state:\n",
    "\n",
    " A prediction is made at the end of each sequence and sequences are independent. State should be reset after each sequence by setting the batch size to 1.\n",
    "\n",
    " A long sequence was split into multiple subsequences (many samples each with many time steps). State should be reset after the network has been exposed to the entire sequence by making the LSTM stateful, turning off the shuffling of subsequences, and resetting the state after each epoch.\n",
    "\n",
    " A very long sequence was split into multiple subsequences (many samples each with many time steps). Training efficiency is more important than the influence of long-term internal state and a batch size of 128 samples was used, after which network weights are updated and state reset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Applications to Models\n",
    "I really want you to understand these models. To that end, this section lists 10 different and varied sequence prediction problems and notes which model may be used to address them. In each explanation, I give an example of a model that can be used to address the problem, but other models can be used if the sequence prediction problem is re-framed. Take these as best suggestions, not unbreakable rules.\n",
    "\n",
    "### Time Series\n",
    "\n",
    " Univariate Time Series Forecasting. This is where you have one series with multiple input time steps and wish to predict one time step beyond the input sequence. This can be implemented as a many-to-one model.\n",
    "\n",
    " Multivariate Time Series Forecasting. This is where you have multiple series with multiple input time steps and wish to predict one time step beyond one or more of the input sequences. This can be implemented as a many-to-one model. Each series is just another input feature.\n",
    "\n",
    " Multi-step Time Series Forecasting: This is where you have one or multiple series with multiple input time steps and wish to predict multiple time steps beyond one or more of the input sequences. This can be implemented as a many-to-many model.\n",
    "\n",
    " Time Series Classification. This is where you have one or multiple series with multiple input time steps as input and wish to output a classification label. This can be implemented as a many-to-one model.\n",
    "\n",
    "### Natural Language Processing\n",
    "\n",
    " Image Captioning. This is where you have one image and wish to generate a textual description. This can be implemented as a one-to-many model.\n",
    "\n",
    " Video Description. This is where you have a sequence of images in a video and wish to generate a textual description. This can be implemented with a many-to-many model.\n",
    "\n",
    " Sentiment Analysis. This is where you have sequences of text as input and you wish to generate a classification label. This can be implemented as a many-to-one model.\n",
    "\n",
    " Speech Recognition. This is where you have a sequence of audio data as input and wish to generate a textual description of what was spoken. This can be implemented with a many-to-many model.\n",
    "\n",
    " Text Translation. This is where you have a sequence of words in one language as input and wish to generate a sequence of words in another language. This can be implemented with a many-to-many model.\n",
    "\n",
    " Text Summarization. This is where you have a document of text as input and wish to create a short textual summary of the document as output. This can be implemented with a many-to-many model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
